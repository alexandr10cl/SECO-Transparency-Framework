{% extends "base.html" %}

{% block head %}
 <link rel="stylesheet" href="{{ url_for('static', filename='css/documentation.css') }}">
 <link rel="stylesheet" href="{{ url_for('static', filename='css/dashboard.css') }}">
 <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined" rel="stylesheet">
{% endblock %}

{% block title %}Documentation{% endblock %}

{% block content %}
    <button class="hamburger-doc" id="hamburgerDoc">
        <span></span>
        <span></span>
        <span></span>
    </button>
    <div class="sidebar-overlay" id="sidebarOverlay"></div>
    <div class="sidebar" id="sidebarDoc">
        <div class="sidebar-header">
            <span class="material-symbols-outlined">article</span>
            <h2>Documentation</h2>
        </div>
        <ul class="sidebar-menu">
            <li><a href="#introduction" class="active">Introduction</a></li>
            <li><a href="#evaluationflow">Evaluation Flow</a></li>
            <li><a href="#chromeextension">Chrome Extension</a></li>
            <li><a href="#uxt">How to use UX-Tracking</a></li>
            <li><a href="#dashboard">Dashboard</a></li>
            <li><a href="#glossary">Glossary of Terms</a></li>
        </ul>
    </div>
    <div class="content">
        <section id="introduction">
            <h1>Introduction</h1>
            <p>
                The <strong>Evaluation Framework for Software Ecosystems Portals (SECO‚ÄëTransP)</strong> is an open framework designed to evaluate and improve transparency practices in 
                software ecosystem (SECO) portals. It combines structured guidelines, hands-on tasks, and developer feedback to provide 
                a holistic view of how transparency is perceived and operationalized in real-world SECOs.
            </p>
            <p>
                By placing developer experience (DX) at the center of the evaluation process, SECO‚ÄëTransP helps SECO platform 
                managers, researchers, and ecosystem participants better understand the conditions that foster trust, clarity, 
                and meaningful contributions.
            </p>

            <h3>üîç What does SECO‚ÄëTransP offer?</h3>
            <ul>
                <li>‚úÖ <strong>10 Transparency Guidelines</strong>, mapped to common SECO procedures, DX factors, and conditioning factors for transparency;</li>
                <li>üß™ <strong>Evaluation Tasks</strong> that simulate real developer interactions with portals;</li>
                <li>üìä <strong>Perception Questionnaire</strong> using Visual Analogue Scales (VAS) to capture subjective experience;</li>
                <li>üõ†Ô∏è <strong>Browser Extension + Evaluation Portal</strong> for task tracking, feedback collection, and dashboard generation;</li>
                <li>üèÖ <strong>Transparency Badges</strong> with levels: High Transparency ‚Ä¢ Moderate Transparency ‚Ä¢ Low Transparency.</li>
            </ul>

            <h3>üë©‚Äçüíª Who is it for?</h3>
            <ul>
                <li><strong>Researchers</strong> seeking a reproducible method to analyze transparency in SECO portals.</li>
                <li><strong>SECO Platform Managers</strong> aiming to identify and enhance transparency practices.</li>
                <li><strong>Developers</strong> interested in contributing to better ecosystems through structured feedback.</li>
            </ul>
            </section>

        <hr>
        <section id="evaluationflow">
            <br>
            <br>
            <br>
              <h1>Evaluation Flow</h1>

                <p>
                The evaluation using the SECO-TransP framework is conducted in six main steps, starting with the initial setup by the portal manager and ending with the aggregated analysis of the developers' feedback. Below is the complete step-by-step guide:</p>
                
                <h2>Step 1 ‚Äì Configure the Evaluation in SECO-TransP</h2>
                <p>
                <strong>At this stage, a portal manager defines the initial parameters of the evaluation:</strong><br>
                ‚Äì Provides basic information about the portal to be evaluated.<br>
                ‚Äì Selects the common software ecosystem procedures to be the focus of the evaluation, such as documentation access, communication channels, governance, etc.<br>
                ‚Äì Then, the manager saves this initial configuration in the system.</p>

                <h2>Step 2 ‚Äì Generate Evaluation Code and Link Tasks</h2>
                <p>
                    <strong>After configuring the evaluation, a portal manager:</strong><br>
                    ‚Äì Generates a unique evaluation code.<br>
                    ‚Äì Shares this code with the participating developers.<br>
                    ‚Äì SECO-TransP automatically generates evaluation tasks based on the selected procedures and links them to the generated code.
                </p>

                <h2>Step 3 ‚Äì Developer Access and Profile Questionnaire</h2>
                <p>
                    <strong>Once the developer receives the evaluation code:</strong><br>
                    ‚Äì Accesses the SECO-TransP extension and enters the provided code.<br>
                    ‚Äì Completes a profile questionnaire with information such as education level, work sector, and development experience ‚Äî data that help contextualize the evaluation results.
                </p>

                <h2>Step 4 ‚Äì Task Execution and Quick Feedback</h2>
                <p>
                    <strong>For each assigned task:</strong><br>
                    ‚Äì A developer indicates whether they were able to complete the task.<br>
                    ‚Äì Optionally, the developer may add a short comment about the task.
                </p>

                <h2>Step 5 ‚Äì Detailed Evaluation by Success Criteria</h2>
                <p>
                    <strong>After all tasks of a procedure are completed:</strong><br>
                    ‚Äì A developer nswers detailed questions based on the Key Success Criteria (KSC) of each guideline linked to that procedure.<br>
                    ‚Äì Each question is rated using a Visual Analogue Scale (VAS) ranging from 0 to 100, allowing the developer to express the extent to which the criterion was perceived as fulfilled.<br>
                    ‚Äì An optional comment field is provided for qualitative observations.<br>
                    ‚Äì At the end of the evaluation, the developer also provides a general satisfaction rating regarding their overall experience using the portal, using a slider scale from 0 to 10.
                </p>

                <h2>Step 6 ‚Äì Calculation and Interpretation of Results</h2>
                <p>
                    <strong>Based on the developers‚Äô responses:</strong><br>
                    ‚Äì Each answer on the VAS is recorded as a numerical value from 0 to 100, where higher values indicate a stronger perception that the criterion was fully met in the portal.<br>
                    ‚Äì The system calculates average scores per criterion, and guideline, and procedure.<br>
                    ‚Äì Guidelines are classified as:<br>
                    &emsp;‚Ä¢ Fulfilled if the average score is ‚â• 75<br>
                    &emsp;‚Ä¢ Partially Fulfilled if the average score is between 50 and 74<br>
                    &emsp;‚Ä¢ Not Fulfilled if the average score is &lt; 50<br>
                    ‚Äì These classifications are used to generate visual badges and support transparency benchmarking across portals.
                </p>

        </section>
        <hr>
        <section id="chromeextension">
            <br>
            <br>
            <br>
            <h1>Chrome extension guide for testers</h1>
            <h2>Overview</h2>
            <p><strong>SECO-TransP</strong> is a browser extension designed to evaluate the transparency of software ecosystem portals. It guides users through a structured process that includes authentication, session sync, task execution, and experience feedback collection focusing on user interaction and perception.</p>
            <p>The extension allows users to:</p>
            <ul>
                <li>Authenticate using a unique evaluation code</li>
                <li>Synchronize with UX-tracking</li>
                <li>Execute guided tasks for evaluation purposes</li>
                <li>Log user navigation in real time</li>
                <li>Provide feedback after each task and at the end</li>
                <li>Submit collected data to a backend server</li>
            </ul>

            <h2>How to Use</h2>

            <h3>1. Extension Installation</h3>
            <ul>
                <li>Download the latest version of the SECO‚ÄëTransP browser extension from the following link: <a href="" target="_blank">SECO‚ÄëTransP Extension</a></li>
                <li>Unzip the folder to a location on your computer</li>
                <li>Go to <code>chrome://extensions/</code> in Google Chrome</li>
                <li>Enable <strong>Developer mode</strong></li>
                <li>Click <strong>‚ÄúLoad unpacked‚Äù</strong> and select the folder with your extension files</li>
                <li>The <strong>SECO-TransP</strong> icon will appear in the toolbar</li>
            </ul>

            <h3>2. Authentication</h3>
            <p>Click on the extension icon to open the authentication screen:</p>
            <ul>
                <li>Enter the provided <strong>7-digit evaluation code</strong></li>
                <li>Click <strong>‚ÄúVerify‚Äù</strong></li>
                <li>If the code is valid, you will be directed to the session synchronization step</li>
                <li>If invalid, an error message will appear</li>
            </ul>

            <h3>3. Session Synchronization</h3>
            <p>Before proceeding:</p>
            <ul>
                <li>Ensure the <strong>UX-Tracking system</strong> is active</li>
                <li>Click <strong>‚ÄúSynchronize‚Äù</strong></li>
                <li>If successful, the evaluation overview page will load</li>
                <li><strong>Note:</strong> The tool works even without UX-Tracking</li>
            </ul>

            <h3>4. Evaluation Stages</h3>

            <h4>Step 1: Profile Questionnaire</h4>
            <p>You will be asked about your academic background, work sector, and experience in software development. These responses help contextualize your feedback.</p>

            <h4>Step 2: Task Execution</h4>
            <p>Tasks will be displayed one at a time. For each:</p>
            <ul>
                <li>Read the task title and description</li>
                <li>Click <strong>‚ÄúStart Task‚Äù</strong></li>
                <li>After completion, select one option:
                <ul>
                    <li><strong>Solved it</strong></li>
                    <li><strong>Not sure</strong></li>
                    <li><strong>Couldn't solve</strong></li>
                </ul>
                </li>
            </ul>

            <h4>Step 3: Task Review</h4>
            <p>After each task, you‚Äôll write a short comment about your experience and click <strong>‚ÄúNext‚Äù</strong> to proceed.</p>

            <h4>Step 4: Process Review</h4>
            <p>Once all tasks of a process are completed, a brief multiple-choice questionnaire will ask if specific objectives were achieved.</p>

            <h4>Step 5: Final Questionnaire</h4>
            <p>Finally, you will provide overall feedback about your experience and rate how you felt using the portal using an emoji scale (from üò´ to ü§©).</p>

            <h3>5. Finishing the Evaluation</h3>
            <ul>
                <li>Click the <strong>‚ÄúFinish Evaluation‚Äù</strong> button</li>
                <li>Your responses will be sent automatically to the backend server</li>
                <li>A success message will confirm completion</li>
            </ul>

            <h2>What Data Is Collected?</h2>
            <ul>
                <li><strong>Task Responses:</strong> title, status, timestamps</li>
                <li><strong>Feedback:</strong> text comments, radio button selections</li>
                <li><strong>Navigation:</strong> URL history and tab switches</li>
                <li><strong>Profile Info:</strong> academic level, sector, experience years</li>
                <li><strong>Emotion:</strong> final emotion scale rating</li>
            </ul>

            <h2>Usage Tips</h2>
            <ul>
                <li>If synchronization fails, ensure UX-Tracking is properly started</li>
                <li>Do not close the popup during the evaluation to prevent data loss</li>
            </ul>
        </section>
        <hr>
        <section id="uxt">
            <br>
            <br>
            <br>
            <h1>How to Use UX-Tracking</h1>
            
            <h2>What is UX-Tracking?</h2>
            <p>UX-Tracking is a tool developed by researchers at the Federal University of Par√° (UFPA) that enables detailed tracking of user interactions on web environments. It features multimodal capture, including facial expressions, mouse movements, keyboard input, and audio, allowing for deeper analysis of user experience (UX).</p>
            <p>This solution is especially useful for usability studies and research, providing valuable data to understand user behavior and emotions during navigation.</p>
            
            <h2>Official UX-Tracking Documentation</h2>
            <p>You can access the official website of the tool through the link below to learn how to use it properly:</p>
            <p><a href="https://uxtracking.netlify.app/" target="_blank">https://uxtracking.netlify.app/</a></p>
        </section>
        <hr>
        <section id="dashboard">
            <br>
            <br>
            <br>
            <h1>Dashboard Functionality</h1>

            <h2>Purpose</h2>
            <p>The dashboard is designed to organize and present data in an accessible and analytical way, allowing portal managers to visually analyze evaluation results, identify strengths and weaknesses of the assessed portal, and make data-driven decisions.</p>

            <h2>Dashboard Structure</h2>
            <p>The dashboard is divided into four main tabs, each focusing on specific aspects of the evaluation:</p>

            <h3>1. Overview</h3>
            <p>Provides a comprehensive summary of the evaluation:</p>
            <ul>
                <li>Overall evaluation score</li>
                <li>Scores by SECO dimension (e.g., governance, communication)</li>
                <li>Scores by Developer Experience (DX) category</li>
                <li>Participant profile charts: academic background and experience</li>
                <li>Word cloud with the most cited terms from comments</li>
                <li>Emotion chart showing developers' overall satisfaction</li>
            </ul>

            <div class="guideline-info-card">
                <h4>Transparency Score ‚Äì Overview</h4>
                <p>
                    The overall transparency score of the portal is calculated based on the average ratings across all evaluated guidelines. 
                    Based on this score, the portal is assigned one of the following transparency levels:
                </p>
                <div class="status-blocks">
                    <div class="status-label">
                    ‚úÖ <strong>High Transparency</strong>: score ‚â• 75
                    </div>
                    <div class="status-label">
                    üü° <strong>Moderate Transparency</strong>: score between 50 and 74
                    </div>
                    <div class="status-label">
                    ‚ùå <strong>Low Transparency</strong>: score &lt; 50
                    </div>
                </div>
            </div>

            <h3>2. Performed Tasks ‚Äî Metrics & Badges</h3>
                <p>This section explains how each task metric is computed and which badge is displayed based on the thresholds.</p>

                <!-- Time Performance -->
                <div class="guideline-info-card">
                    <h4>Metric: Time Performance</h4>
                    <p><strong>Definition.</strong> Average completion time for a task.</p>
                    <p><strong>How it‚Äôs calculated.</strong> Mean duration among participants who completed the task (failed or abandoned attempts are excluded). If no participant completed the task, the time metric is reported as N/A.</p>
                    <p><strong>Badges.</strong> The following badge is shown based on the average time:</p>
                    
                    <div class="status-blocks">
                        <div class="status-label">
                            <strong>Fast</strong>: ‚â§ 30s
                        </div>
                        <div class="status-label">
                            <strong>Moderate</strong>: 31‚Äì60s
                        </div>
                        <div class="status-label">
                            <strong>Slow</strong>: 61‚Äì120s
                        </div>
                        <div class="status-label">
                            <strong>Very Slow</strong>: &gt; 120s
                        </div>
                    </div>
                    
                    <details>
                    <summary><strong>Examples</strong></summary>
                    <ul>
                        <li>Average time = <strong>24s</strong> ‚Üí <span class="badge badge-excellent">Fast</span></li>
                        <li>Average time = <strong>52s</strong> ‚Üí <span class="badge badge-good">Moderate</span></li>
                        <li>Average time = <strong>88s</strong> ‚Üí <span class="badge badge-warning">Slow</span></li>
                        <li>Average time = <strong>143s</strong> ‚Üí <span class="badge badge-critical">Very Slow</span></li>
                    </ul>
                    </details>
                </div>

                <!-- Success Rate -->
                <div class="guideline-info-card">
                    <h4>Metric: Success Rate</h4>
                    <p><strong>Definition.</strong> Percentage of participants who completed the task.</p>
                    <p><strong>How it‚Äôs calculated.</strong> <code>success rate = (completed / attempted) √ó 100</code>.</p>
                    <p><strong>Badges.</strong> The following badge is shown based on the success rate:</p>
                    <div class="status-blocks">
                        <div class="status-label">
                            <strong>High Success</strong>: ‚â• 90%
                        </div>
                        <div class="status-label">
                            <strong>Good Success</strong>: 70‚Äì89%
                        </div>
                        <div class="status-label">
                            <strong>Low Success</strong>: 50‚Äì69%
                        </div>
                        <div class="status-label">
                            <strong>Poor Success</strong>: &lt; 50%
                        </div>
                    </div>
                    <details>
                    <summary><strong>Examples</strong></summary>
                    <ul>
                        <li>Success rate = <strong>93%</strong> ‚Üí <span class="badge badge-excellent">High Success</span></li>
                        <li>Success rate = <strong>78%</strong> ‚Üí <span class="badge badge-good">Good Success</span></li>
                        <li>Success rate = <strong>63%</strong> ‚Üí <span class="badge badge-warning">Low Success</span></li>
                        <li>Success rate = <strong>41%</strong> ‚Üí <span class="badge badge-critical">Poor Success</span></li>
                    </ul>
                    </details>
                </div>

                <!-- Attention rule -->
                <div class="guideline-info-card">
                    <h4>Attention Badge</h4>
                    <p><strong>Rule.</strong> The ‚ÄúNeeds Attention‚Äù badge is displayed when <em>both</em> conditions are true:</p>
                    <ul>
                    <li>Average time &gt; 120s</li>
                    <li>Success rate &lt; 50%.</li>
                    </ul>
                    <p>This combination indicates that the task is taking too long and many participants are failing.</p>
                    <p><strong>Badge.</strong> <span class="attention-badge-small">Needs Attention</span></p>
                    <details>
                    <summary><strong>Examples</strong></summary>
                    <ul>
                        <li>Avg time = <strong>151s</strong>, success = <strong>42%</strong> ‚Üí <span class="attention-badge-small">Needs Attention</span></li>
                        <li>Avg time = <strong>132s</strong>, success = <strong>67%</strong> ‚Üí <em>No attention badge</em> (time is high, but success is not &lt; 50%)</li>
                        <li>Avg time = <strong>90s</strong>, success = <strong>38%</strong> ‚Üí <em>No attention badge</em> (success is low, but time ‚â§ 120s)</li>
                    </ul>
                    </details>
                </div>

                <!-- Example Task Card combining all -->
                <div class="guideline-info-card">
                    <h4>Example Task Card</h4>
                    <p>Below is how a task appears with its computed metrics and corresponding badges.</p>
                        <h5>Task T1 ‚Äî Locate the API Documentation</h5>
                        <div class="status-blocks" aria-label="Task badges">
                            <!-- Example badges (static for documentation) -->
                            <span class="badge badge-warning">Slow</span>
                            <span class="badge badge-critical">Poor Success</span>
                            <span class="attention-badge-small">Needs Attention</span>
                        </div>
                        <ul>
                            <li><strong>Average completion time:</strong> 01:32 (mm:ss)</li>
                            <li><strong>Success rate:</strong> 47%</li>
                            <li><strong>Average VAS:</strong> 71/100</li>
                        </ul>
                        <details>
                            <summary><strong>Participant comments (examples)</strong></summary>
                            <ul>
                            <li>‚ÄúDocs were easy to find once I used search.‚Äù</li>
                            <li>‚ÄúNavigation labels could be clearer.‚Äù</li>
                            <li>‚ÄúGreat examples, but missing SDK snippet.‚Äù</li>
                            </ul>
                        </details>
                </div>

            <h3>3. Hotspots (Heatmaps)</h3>
            <p>When multimodal tracking via UX-Tracking is enabled, this section presents heatmaps per task:</p>
            <ul>
                <li>Visualization of the most clicked or hovered areas</li>
                <li>Helps identify zones of interest, confusion, or abandonment during task execution</li>
            </ul>
            <p>This visual layer complements the quantitative data with behavioral insights.</p>

            <h3>4. Guidelines Score</h3>
            <p>
            The <strong>Guidelines Score</strong> section displays the evaluation results based on the transparency KSC guidelines' <em>Key Success Criteria ()</em>. Each guideline is assessed through developer responses, which are translated into scores and statuses.
            </p>

            <p>
            Each evaluated guideline is presented with:
            </p>

            <div class="guideline-info-card">
                <h4>Evaluated Guidelines</h4>
                <p>A complete list of the guidelines selected for assessment during the evaluation process.</p>
            </div>

            <div class="guideline-info-card">
                <h4>Average Score</h4>
                <p>The system calculates an average score for each guideline based on developer responses to its associated KSC. Each response is given on a VAS from 0 to 100, where higher values indicate a stronger perception that the criterion was fully met in the portal.
                    The average is computed across all KSC linked to the guideline and is used to determine the corresponding fulfillment badge classification.</p>
            </div>

            <div class="guideline-info-card">
                <h4>Status Classification</h4>
                <p>According to the average score, each guideline is classified as:</p>
                <div class="status-blocks">
                    <div class="status-label"><strong>Fulfilled</strong>: 75 or higher</div>
                    <div class="status-label"><strong>Partially Fulfilled</strong>: between 50 and 74</div>
                    <div class="status-label"><strong>Not Fulfilled</strong>: below 50</div>
                </div>
            </div>

            <div class="guideline-info-card">
                <h4>KSC Visualization</h4>
                <p>Each guideline includes a breakdown of its KSC, showing individual scores and status indicators. This helps identify precisely where the portal meets or falls short on transparency expectations.</p>
            </div>

            <p>
            This section is essential for diagnosing specific transparency issues and guiding improvements in the software ecosystem portal.
            </p>
        </section>

         <section id="glossary">
            <div class="container">
                <h1>Glossary of Terms</h1>
                <p>This glossary provides definitions for key terms used throughout the SECO‚ÄëTransP framework.</p>

                <dl>
                <dt><strong>Conditioning Factors for Transparency</strong></dt>
                <dd>
                    Elements, characteristics, or actions that are necessary but not sufficient for the transparency of software ecosystems. A recent study identified and analyzed these factors through a systematic mapping and field study, structuring them around themes like access to information, communication channels, and requirements engineering (<a href="https://doi.org/10.5753/jserd.2024.4086" target="_blank">Zacarias et‚ÄØal.,‚ÄØ2024</a>).
                </dd>

                <dt><strong>Developer Experience</strong></dt>
                <dd>
                    A broad concept that captures how developers feel about, think about, and value their interactions with tools, platforms, processes, and documentation. It encompasses emotional, cognitive, and practical aspects that influence developer satisfaction, efficiency, and motivation.
                </dd>

                <dt><strong>Developer Experience Factor</strong></dt>
                <dd>
                    A specific circumstance, condition, or influence that affects how developers perceive and experience their interactions within a software ecosystem. These factors can be technical, cognitive, social, or emotional, and collectively shape the overall developer experience. A recent study identified 27 such factors, highlighting especially the influence of ‚Äúfinancial costs for using the platform‚Äù, ‚Äúdesired technical resources for development‚Äù, ‚Äúlow barriers to entry into the application market‚Äù, and ‚Äúmore financial gains‚Äù, as key determinants for third‚Äëparty developers when deciding to adopt and continue contributing to a SECO (<a href="https://doi.org/10.1016/j.jss.2025.112549" target="_blank">Zacarias et‚ÄØal.,‚ÄØ2025</a>).
                </dd>

                <dt><strong>Evaluation Task</strong></dt>
                <dd>
                    A practical task performed by developers in the portal to simulate real interactions and assess the level of transparency in a specific procedure.
                </dd>

                <dt><strong>Key Success Criterion</strong></dt>
                <dd>
                    A concrete indicator used to evaluate whether a transparency guideline is effectively fulfilled from a developer's point of view.
                </dd>

                <dt><strong>Software Ecosystem</strong></dt>
                <dd>
                    A set of actors that function as a unit and their relationships and interactions within a distributed market of software and services. These relationships are typically centered around a common technological platform, allowing the exchange of information, resources, and artifacts.
                </dd>

                <dt><strong>Software Ecosystem Portal</strong></dt>
                <dd>
                    A web-based interface that provides access to artifacts, tools, documentation, and services offered by a common technological platform within a software ecosystem.
                </dd>

                <dt><strong>Transparency</strong></dt>
                <dd>
                    A condition in which information about capabilities, priorities, and behaviors is openly available, enabling informed decision‚Äëmaking by stakeholders and reducing inefficiencies caused by information asymmetry.
                </dd>

                <dt><strong>Transparency Badge</strong></dt>
                <dd>
                    A visual indicator assigned to a guideline based on evaluation scores. The three levels are: <strong>High Transparency</strong> (‚â•¬†75), <strong>Moderate Transparency</strong> (50‚Äì74), and <strong>Low Transparency</strong> (<¬†50).
                </dd>

                <dt><strong>Transparency Guideline</strong></dt>
                <dd>
                    A recommended practice aimed at increasing visibility, clarity, and fairness in specific areas of the SECO portal (e.g., source code access, governance, contribution policies).
                </dd>

                <dt><strong>VAS (Visual Analogue Scale)</strong></dt>
                <dd>
                    A type of measurement scale where developers indicate their perception by selecting a position along a continuous line, typically from 0 to 100.
                </dd>
                </dl>
            </div>
        </section>



    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Mobile sidebar toggle
            const hamburgerDoc = document.getElementById('hamburgerDoc');
            const sidebarDoc = document.getElementById('sidebarDoc');
            const sidebarOverlay = document.getElementById('sidebarOverlay');
            
            if (hamburgerDoc) {
                hamburgerDoc.addEventListener('click', function() {
                    hamburgerDoc.classList.toggle('active');
                    sidebarDoc.classList.toggle('active');
                    sidebarOverlay.classList.toggle('active');
                });
            }
            
            // Close sidebar when clicking overlay
            if (sidebarOverlay) {
                sidebarOverlay.addEventListener('click', function() {
                    hamburgerDoc.classList.remove('active');
                    sidebarDoc.classList.remove('active');
                    sidebarOverlay.classList.remove('active');
                });
            }
            
            // Active link handling
            const sidebarLinks = document.querySelectorAll(".sidebar-menu a");
            sidebarLinks.forEach(link => {
                link.addEventListener("click", function() {
                    sidebarLinks.forEach(l => l.classList.remove("active"));
                    this.classList.add("active");
                    
                    // Close mobile sidebar after clicking a link
                    if (window.innerWidth < 1024) {
                        hamburgerDoc.classList.remove('active');
                        sidebarDoc.classList.remove('active');
                        sidebarOverlay.classList.remove('active');
                    }
                });
            });
        });
    </script>
{% endblock %}
